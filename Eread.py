import streamlit as st
import feedparser
import sqlite3
import pandas as pd
import datetime
import random

# --- è¨­å®šé é¢ ---
st.set_page_config(page_title="Bio-Science Reader", layout="wide")

# --- ğŸ¨ è­·çœ¼æ¨¡å¼ (Google Style Dark Mode) ---
def apply_google_dark_mode():
    st.markdown("""
        <style>
        .stApp { background-color: #202124; }
        section[data-testid="stSidebar"] { background-color: #171717; }
        h1, h2, h3, h4, h5, h6, p, li, label, .stMarkdown, .stText { color: #E8EAED !important; }
        a { color: #8AB4F8 !important; }
        div[data-testid="stVerticalBlockBorderWrapper"] > div { background-color: #303134; border-color: #3c4043; }
        button { border-color: #5f6368 !important; color: #E8EAED !important; }
        button:hover { border-color: #8AB4F8 !important; color: #8AB4F8 !important; }
        div[data-testid="stDataFrame"] { background-color: #303134; }
        </style>
    """, unsafe_allow_html=True)

# --- è³‡æ–™åº«è™•ç† (SQLite) ---
def init_db():
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    # 1. é–±è®€ç´€éŒ„è¡¨
    c.execute('''
        CREATE TABLE IF NOT EXISTS logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date DATE,
            title TEXT,
            category TEXT
        )
    ''')
    # 2. å–®å­—åº«è¡¨
    c.execute('''
        CREATE TABLE IF NOT EXISTS vocabulary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date DATE,
            word TEXT,
            meaning TEXT,
            note TEXT
        )
    ''')
    # 3. [NEW] æ¯æ—¥å›ºå®šæ–‡ç« è¡¨
    # ç”¨ä¾†å„²å­˜æ¯å¤©ç³»çµ±æŒ‘é¸çš„é‚£ 3 ç¯‡ï¼Œç¢ºä¿ç•¶å¤©ä¸æœƒè®Š
    c.execute('''
        CREATE TABLE IF NOT EXISTS daily_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date DATE,
            title TEXT,
            link TEXT,
            summary TEXT,
            category TEXT,
            published TEXT
        )
    ''')
    conn.commit()
    conn.close()

# --- è³‡æ–™åº«æ“ä½œåŠŸèƒ½ ---
def log_reading(title, category):
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    today = datetime.date.today()
    # é¿å…é‡è¤‡æ‰“å¡åŒä¸€ç¯‡ (é¸æ“‡æ€§åŠŸèƒ½)
    c.execute("SELECT * FROM logs WHERE date = ? AND title = ?", (today, title))
    if not c.fetchone():
        c.execute('INSERT INTO logs (date, title, category) VALUES (?, ?, ?)', (today, title, category))
        conn.commit()
        st.success(f"å·²è¨˜éŒ„é–±è®€ï¼š{title}")
    else:
        st.warning(f"é€™ç¯‡ä»Šå¤©å·²ç¶“è®€éå›‰ï¼š{title}")
    conn.close()

def add_vocab(word, meaning, note):
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    today = datetime.date.today()
    c.execute('INSERT INTO vocabulary (date, word, meaning, note) VALUES (?, ?, ?, ?)', (today, word, meaning, note))
    conn.commit()
    conn.close()
    st.sidebar.success(f"å·²å„²å­˜ï¼š{word}")

def get_reading_stats():
    conn = sqlite3.connect('reading_log.db')
    df = pd.read_sql_query("SELECT date, count(*) as count FROM logs GROUP BY date ORDER BY date", conn)
    conn.close()
    return df

def get_vocab_list():
    conn = sqlite3.connect('reading_log.db')
    df = pd.read_sql_query("SELECT date as 'æ—¥æœŸ', word as 'å–®å­—', meaning as 'ä¸­æ–‡æ„æ€', note as 'å‚™è¨»' FROM vocabulary ORDER BY id DESC", conn)
    conn.close()
    return df

# --- [æ ¸å¿ƒä¿®æ”¹] å–å¾—ä»Šæ—¥æ–‡ç«  (å›ºå®šç‰ˆ) ---
def get_todays_articles_fixed():
    today = datetime.date.today()
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    
    # 1. å…ˆæŸ¥è³‡æ–™åº«ï¼šä»Šå¤©æ˜¯å¦å·²ç¶“ç”¢ç”Ÿéæ–‡ç« ï¼Ÿ
    c.execute("SELECT title, link, summary, category, published FROM daily_articles WHERE date = ?", (today,))
    rows = c.fetchall()
    
    # 2. å¦‚æœè³‡æ–™åº«æœ‰è³‡æ–™ (ä»£è¡¨ä»Šå¤©å·²ç¶“ç”¢ç”Ÿéäº†)ï¼Œç›´æ¥å›å‚³é€™äº›æ–‡ç« 
    if rows:
        conn.close()
        articles = []
        for row in rows:
            articles.append({
                'title': row[0],
                'link': row[1],
                'summary': row[2],
                'category': row[3],
                'published': row[4]
            })
        return articles

    # 3. å¦‚æœè³‡æ–™åº«æ²’æœ‰è³‡æ–™ (ä»£è¡¨æ˜¯ä»Šå¤©ç¬¬ä¸€æ¬¡é–‹å•Ÿ)ï¼Œå»æŠ“ RSS ä¸¦éš¨æ©Ÿé¸ 3 ç¯‡å­˜å…¥
    else:
        # --- æŠ“å– RSS é‚è¼¯ ---
        rss_urls = [
            ('Biology', 'https://www.sciencedaily.com/rss/plants_animals/biology.xml'),
            ('Health', 'https://www.sciencedaily.com/rss/health_medicine.xml'),
            ('Science', 'https://www.sciencedaily.com/rss/top/science.xml')
        ]
        pool = []
        for category, url in rss_urls:
            try:
                feed = feedparser.parse(url)
                # æ¯å€‹åˆ†é¡å¤šæŠ“ä¸€é» (å‰ 5 ç¯‡) ä¾†åšéš¨æ©Ÿæ± 
                for entry in feed.entries[:5]:
                    pool.append({
                        'title': entry.title,
                        'link': entry.link,
                        'summary': entry.summary,
                        'category': category,
                        'published': entry.get('published', 'Unknown')
                    })
            except:
                continue
        
        # å¾æ± ä¸­éš¨æ©Ÿé¸ 3 ç¯‡
        selected_articles = []
        if len(pool) >= 3:
            selected_articles = random.sample(pool, 3)
        else:
            selected_articles = pool
        
        # --- å­˜å…¥è³‡æ–™åº« (ç¶å®šä»Šå¤©æ—¥æœŸ) ---
        for art in selected_articles:
            c.execute('''
                INSERT INTO daily_articles (date, title, link, summary, category, published)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (today, art['title'], art['link'], art['summary'], art['category'], art['published']))
        
        conn.commit()
        conn.close()
        return selected_articles

# --- ä¸»ç¨‹å¼é‚è¼¯ ---
def main():
    init_db()

    # --- å´é‚Šæ¬„ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        dark_mode = st.toggle("ğŸŒ™ è­·çœ¼æ¨¡å¼ (Google Dark)", value=False)
        if dark_mode:
            apply_google_dark_mode()
        st.divider()
        st.header("ğŸ“ å–®å­—ç­†è¨˜æœ¬")
        with st.form("vocab_form", clear_on_submit=True):
            input_word = st.text_input("è‹±æ–‡å–®å­—")
            input_meaning = st.text_input("ä¸­æ–‡æ„æ€")
            input_note = st.text_area("å‚™è¨»", height=80)
            submitted = st.form_submit_button("ğŸ’¾ å„²å­˜")
            if submitted and input_word:
                add_vocab(input_word, input_meaning, input_note)
        st.markdown("---")
        st.caption("Daily Bio-Science Reader")

    # --- ä¸»ç•«é¢ ---
    st.title("ğŸ§¬ Daily Bio-Science Reader")
    st.markdown(f"**{datetime.date.today()}** ä»Šæ—¥ç²¾é¸æ–‡ç«  (24å°æ™‚å…§å›ºå®š)")
    st.divider()

    # 1. æ–‡ç« å€å¡Š (ä½¿ç”¨æ–°çš„å›ºå®šå‡½æ•¸)
    st.header("ğŸ“– ä»Šæ—¥é–±è®€ä»»å‹™")
    
    # é€™è£¡ç›´æ¥å‘¼å«å‡½æ•¸ï¼Œä¸å†ä¾è³´ session_state ä¾†ã€Œæš«å­˜ã€ï¼Œ
    # å› ç‚ºç¾åœ¨æ˜¯ç”±è³‡æ–™åº«ä¾†ã€Œæ°¸ä¹…å„²å­˜ã€ä»Šå¤©çš„é¸æ“‡ã€‚
    daily_articles = get_todays_articles_fixed()

    cols = st.columns(3)
    # è™•ç†å¯èƒ½æŠ“ä¸åˆ°æ–‡ç« çš„æƒ…æ³
    if not daily_articles:
        st.error("ç„¡æ³•å–å¾—æ–‡ç« ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ–ç¨å¾Œå†è©¦ã€‚")
    else:
        for i, article in enumerate(daily_articles):
            with cols[i]:
                with st.container(border=True):
                    st.subheader(article['title'])
                    st.caption(f"ğŸ·ï¸ {article['category']}")
                    st.write(article['summary'])
                    st.markdown(f"[ğŸ‘‰ é–±è®€å…¨æ–‡]({article['link']})")
                    
                    if st.button(f"âœ… å®Œæˆ", key=f"btn_{i}", use_container_width=True):
                        log_reading(article['title'], article['category'])
                        st.rerun()

    st.divider()

    # 2. æ•¸æ“šèˆ‡å–®å­—åº«å€å¡Š
    tab1, tab2 = st.tabs(["ğŸ“ˆ ç´¯ç©æˆå°±åœ–è¡¨", "ğŸ”¤ æˆ‘çš„å–®å­—åº«"])

    with tab1:
        df_stats = get_reading_stats()
        if not df_stats.empty:
            df_stats['date'] = pd.to_datetime(df_stats['date'])
            df_stats['cumulative'] = df_stats['count'].cumsum()
            st.area_chart(df_stats, x='date', y='cumulative', color="#8AB4F8")
            st.metric("ç¸½é–±è®€ç¯‡æ•¸", df_stats['count'].sum())
        else:
            st.info("å°šç„¡é–±è®€ç´€éŒ„ï¼ŒåŠ æ²¹ï¼")

    with tab2:
        df_vocab = get_vocab_list()
        if not df_vocab.empty:
            st.dataframe(
                df_vocab, 
                use_container_width=True,
                hide_index=True,
            )
        else:
            st.info("å´é‚Šæ¬„å¯ä»¥æ–°å¢å–®å­—å–”ï¼")

if __name__ == "__main__":
    main()
