import streamlit as st
import feedparser
import sqlite3
import pandas as pd
import datetime
import random

# --- è¨­å®šé é¢ ---
st.set_page_config(page_title="Bio-Science Reader", layout="wide")

# --- ğŸ¨ è­·çœ¼æ¨¡å¼ (Google Style Dark Mode) ---
def apply_google_dark_mode():
    st.markdown("""
        <style>
        /* 1. æ•´é«”èƒŒæ™¯ - Google Dark Grey */
        .stApp {
            background-color: #202124;
        }
        
        /* 2. å´é‚Šæ¬„èƒŒæ™¯ */
        section[data-testid="stSidebar"] {
            background-color: #171717; 
        }

        /* 3. æ–‡å­—é¡è‰² - Google Off-white */
        h1, h2, h3, h4, h5, h6, p, li, label, .stMarkdown, .stText {
            color: #E8EAED !important;
        }
        
        /* 4. é€£çµé¡è‰² - Google Blue */
        a {
            color: #8AB4F8 !important;
        }

        /* 5. å¡ç‰‡/å®¹å™¨èƒŒæ™¯ - Google Surface Color */
        /* é‡å° st.container(border=True) çš„æ¨£å¼è¦†å¯« */
        div[data-testid="stVerticalBlockBorderWrapper"] > div {
            background-color: #303134;
            border-color: #3c4043;
        }
        
        /* 6. æŒ‰éˆ•æ¨£å¼å¾®èª¿ */
        button {
            border-color: #5f6368 !important;
            color: #E8EAED !important;
        }
        button:hover {
            border-color: #8AB4F8 !important;
            color: #8AB4F8 !important;
        }
        
        /* 7. è¡¨æ ¼/Dataframe æ–‡å­—ä¿®æ­£ */
        div[data-testid="stDataFrame"] {
            background-color: #303134; 
        }
        </style>
    """, unsafe_allow_html=True)

# --- è³‡æ–™åº«è™•ç† (SQLite) ---
def init_db():
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date DATE,
            title TEXT,
            category TEXT
        )
    ''')
    c.execute('''
        CREATE TABLE IF NOT EXISTS vocabulary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date DATE,
            word TEXT,
            meaning TEXT,
            note TEXT
        )
    ''')
    conn.commit()
    conn.close()

def log_reading(title, category):
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    today = datetime.date.today()
    c.execute('INSERT INTO logs (date, title, category) VALUES (?, ?, ?)', (today, title, category))
    conn.commit()
    conn.close()
    st.success(f"å·²è¨˜éŒ„é–±è®€ï¼š{title}")

def add_vocab(word, meaning, note):
    conn = sqlite3.connect('reading_log.db')
    c = conn.cursor()
    today = datetime.date.today()
    c.execute('INSERT INTO vocabulary (date, word, meaning, note) VALUES (?, ?, ?, ?)', (today, word, meaning, note))
    conn.commit()
    conn.close()
    st.sidebar.success(f"å·²å„²å­˜ï¼š{word}")

def get_reading_stats():
    conn = sqlite3.connect('reading_log.db')
    df = pd.read_sql_query("SELECT date, count(*) as count FROM logs GROUP BY date ORDER BY date", conn)
    conn.close()
    return df

def get_vocab_list():
    conn = sqlite3.connect('reading_log.db')
    df = pd.read_sql_query("SELECT date as 'æ—¥æœŸ', word as 'å–®å­—', meaning as 'ä¸­æ–‡æ„æ€', note as 'å‚™è¨»' FROM vocabulary ORDER BY id DESC", conn)
    conn.close()
    return df

# --- æŠ“å–æ–‡ç« åŠŸèƒ½ ---
def get_articles():
    rss_urls = [
        ('Biology', 'https://www.sciencedaily.com/rss/plants_animals/biology.xml'),
        ('Health', 'https://www.sciencedaily.com/rss/health_medicine.xml'),
        ('Science', 'https://www.sciencedaily.com/rss/top/science.xml')
    ]
    articles = []
    for category, url in rss_urls:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:3]:
                articles.append({
                    'title': entry.title,
                    'link': entry.link,
                    'summary': entry.summary,
                    'category': category,
                    'published': entry.get('published', 'Unknown')
                })
        except:
            continue
    
    if len(articles) >= 3:
        return random.sample(articles, 3)
    else:
        return articles

# --- ä¸»ç¨‹å¼é‚è¼¯ ---
def main():
    init_db()

    # --- å´é‚Šæ¬„ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # [NEW] è­·çœ¼æ¨¡å¼é–‹é—œ
        # é è¨­ç‚º False (äº®è‰²æ¨¡å¼)ï¼Œé–‹å•Ÿå‰‡è®Šç‚º True
        dark_mode = st.toggle("ğŸŒ™ è­·çœ¼æ¨¡å¼ (Google Dark)", value=False)
        if dark_mode:
            apply_google_dark_mode()

        st.divider()
        
        st.header("ğŸ“ å–®å­—ç­†è¨˜æœ¬")
        with st.form("vocab_form", clear_on_submit=True):
            input_word = st.text_input("è‹±æ–‡å–®å­—")
            input_meaning = st.text_input("ä¸­æ–‡æ„æ€")
            input_note = st.text_area("å‚™è¨»", height=80)
            submitted = st.form_submit_button("ğŸ’¾ å„²å­˜")
            if submitted and input_word:
                add_vocab(input_word, input_meaning, input_note)
        
        st.markdown("---")
        st.caption("Daily Bio-Science Reader")

    # --- ä¸»ç•«é¢ ---
    st.title("ğŸ§¬ Daily Bio-Science Reader")
    st.markdown("æ¯å¤© 3 ç¯‡ï¼Œç´¯ç©ç§‘å­¸é–±è®€é‡ï¼Œæ“´å……å–®å­—åº«ï¼")
    st.divider()

    # 1. æ–‡ç« å€å¡Š
    st.header("ğŸ“– ä»Šæ—¥é–±è®€ä»»å‹™")
    if 'articles' not in st.session_state:
        st.session_state.articles = get_articles()

    cols = st.columns(3)
    for i, article in enumerate(st.session_state.articles):
        with cols[i]:
            # ä½¿ç”¨ container è®“å¡ç‰‡æ›´æ˜é¡¯
            with st.container(border=True):
                st.subheader(article['title'])
                st.caption(f"ğŸ·ï¸ {article['category']}")
                st.write(article['summary'])
                st.markdown(f"[ğŸ‘‰ é–±è®€å…¨æ–‡]({article['link']})")
                
                # æŒ‰éˆ•
                if st.button(f"âœ… å®Œæˆ", key=f"btn_{i}", use_container_width=True):
                    log_reading(article['title'], article['category'])
                    st.rerun()

    st.divider()

    # 2. æ•¸æ“šèˆ‡å–®å­—åº«å€å¡Š
    tab1, tab2 = st.tabs(["ğŸ“ˆ ç´¯ç©æˆå°±åœ–è¡¨", "ğŸ”¤ æˆ‘çš„å–®å­—åº«"])

    with tab1:
        df_stats = get_reading_stats()
        if not df_stats.empty:
            df_stats['date'] = pd.to_datetime(df_stats['date'])
            df_stats['cumulative'] = df_stats['count'].cumsum()
            st.area_chart(df_stats, x='date', y='cumulative', color="#8AB4F8") # æ”¹ç”¨ Google Blue
            st.metric("ç¸½é–±è®€ç¯‡æ•¸", df_stats['count'].sum())
        else:
            st.info("å°šç„¡é–±è®€ç´€éŒ„ï¼ŒåŠ æ²¹ï¼")

    with tab2:
        df_vocab = get_vocab_list()
        if not df_vocab.empty:
            st.dataframe(
                df_vocab, 
                use_container_width=True,
                hide_index=True,
            )
        else:
            st.info("å´é‚Šæ¬„å¯ä»¥æ–°å¢å–®å­—å–”ï¼")

if __name__ == "__main__":
    main()
